# Database Integration & Photo Storage Solutions
## Browser Automation Testing Framework

**Date:** October 17, 2025  
**Purpose:** Compare solutions for persistent storage, test history, and screenshot management

---

## Table of Contents
1. [Current Architecture Analysis](#current-architecture-analysis)
2. [Database Solutions Comparison](#database-solutions-comparison)
3. [Photo/Screenshot Storage Comparison](#photoscreenshot-storage-comparison)
4. [Recommended Architectures](#recommended-architectures)
5. [Implementation Examples](#implementation-examples)
6. [Cost Analysis](#cost-analysis)
7. [Migration Strategy](#migration-strategy)

---

## Current Architecture Analysis

### Current State
```
Storage:
‚îú‚îÄ‚îÄ activity_log.json (test recordings)
‚îú‚îÄ‚îÄ screenshots/ (PNG files, local disk)
‚îî‚îÄ‚îÄ replay_report.html (test results)

Issues:
‚ùå No persistence across sessions
‚ùå No test history or trends
‚ùå Hard to query/analyze
‚ùå Screenshots fill up disk
‚ùå No multi-user support
‚ùå No concurrent test runs
‚ùå Limited reporting capabilities
```

### What We Need
1. **Store test definitions** (recorded activities)
2. **Store test results** (pass/fail, duration, errors)
3. **Store screenshots** (before/after, failures)
4. **Query capabilities** (find flaky tests, trends)
5. **Multi-user support** (team collaboration)
6. **API access** (integrate with CI/CD)

---

## Database Solutions Comparison

### 1. **SQLite** üì¶

#### Overview
Lightweight, file-based SQL database embedded in Python.

#### Pros ‚úÖ
- **Zero setup** - Single file, no server needed
- **Built into Python** - No dependencies
- **Fast for small-medium data** - Good performance <100GB
- **ACID compliant** - Data integrity guaranteed
- **Easy backup** - Just copy the .db file
- **Perfect for local dev** - Each developer has own DB
- **SQL support** - Complex queries possible

#### Cons ‚ùå
- **Single writer** - No concurrent writes
- **Limited scalability** - Not for massive data
- **No built-in replication** - Hard to share across team
- **File-based** - Can't access remotely easily
- **Manual locking** - Race conditions possible

#### Best For
‚úÖ Local development  
‚úÖ Single user  
‚úÖ Prototyping  
‚úÖ Small teams (<5)  
‚úÖ <100k tests/year

#### Schema Example
```sql
CREATE TABLE test_runs (
    id INTEGER PRIMARY KEY,
    test_name TEXT,
    timestamp DATETIME,
    status TEXT,
    duration REAL,
    error_message TEXT
);

CREATE TABLE test_steps (
    id INTEGER PRIMARY KEY,
    test_run_id INTEGER,
    step_number INTEGER,
    action TEXT,
    success BOOLEAN,
    screenshot_path TEXT,
    FOREIGN KEY (test_run_id) REFERENCES test_runs(id)
);

CREATE TABLE screenshots (
    id INTEGER PRIMARY KEY,
    test_run_id INTEGER,
    step_number INTEGER,
    type TEXT, -- 'before', 'after', 'failure'
    file_path TEXT,
    captured_at DATETIME,
    file_size INTEGER
);
```

#### Code Example
```python
import sqlite3
from datetime import datetime

class TestDatabase:
    def __init__(self, db_path='test_automation.db'):
        self.conn = sqlite3.connect(db_path)
        self.create_tables()
    
    def save_test_run(self, test_name, status, duration, error=None):
        cursor = self.conn.cursor()
        cursor.execute("""
            INSERT INTO test_runs (test_name, timestamp, status, duration, error_message)
            VALUES (?, ?, ?, ?, ?)
        """, (test_name, datetime.now(), status, duration, error))
        self.conn.commit()
        return cursor.lastrowid
    
    def get_flaky_tests(self, days=7):
        cursor = self.conn.cursor()
        cursor.execute("""
            SELECT test_name, 
                   COUNT(*) as total_runs,
                   SUM(CASE WHEN status='pass' THEN 1 ELSE 0 END) as passes,
                   SUM(CASE WHEN status='fail' THEN 1 ELSE 0 END) as fails
            FROM test_runs
            WHERE timestamp > datetime('now', '-' || ? || ' days')
            GROUP BY test_name
            HAVING fails > 0 AND passes > 0
            ORDER BY (fails * 1.0 / total_runs) DESC
        """, (days,))
        return cursor.fetchall()
```

#### Cost
**FREE** ‚úÖ

---

### 2. **PostgreSQL** üêò

#### Overview
Full-featured, open-source relational database.

#### Pros ‚úÖ
- **Production-ready** - Used by giants (Apple, Netflix)
- **ACID compliant** - Strongest consistency
- **Concurrent writes** - Multiple users/processes
- **Rich data types** - JSON, arrays, custom types
- **Full-text search** - Built-in search capabilities
- **Advanced features** - Triggers, stored procedures
- **Great tools** - pgAdmin, DataGrip, DBeaver
- **Horizontal scaling** - With extensions (Citus)
- **JSON support** - Store test data as JSON
- **Point-in-time recovery** - Advanced backup

#### Cons ‚ùå
- **Requires server** - Setup/maintenance overhead
- **Memory hungry** - Needs decent resources
- **Overkill for small projects** - Complex for simple needs
- **Learning curve** - More to learn than SQLite
- **Costs** - Server hosting (can be minimal)

#### Best For
‚úÖ Production systems  
‚úÖ Teams (5-100+)  
‚úÖ High concurrency  
‚úÖ Complex queries  
‚úÖ >100k tests/year  
‚úÖ Compliance requirements

#### Schema Example
```sql
CREATE TABLE test_runs (
    id SERIAL PRIMARY KEY,
    test_name VARCHAR(255),
    timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    status VARCHAR(50),
    duration NUMERIC(10, 2),
    error_message TEXT,
    metadata JSONB, -- Store flexible test data
    created_by VARCHAR(100)
);

CREATE INDEX idx_test_runs_timestamp ON test_runs(timestamp);
CREATE INDEX idx_test_runs_status ON test_runs(status);
CREATE INDEX idx_test_runs_metadata ON test_runs USING GIN(metadata);

-- Query JSON metadata
SELECT * FROM test_runs 
WHERE metadata->>'browser' = 'chrome'
AND metadata->>'viewport' = '1920x1080';
```

#### Code Example
```python
import psycopg2
from psycopg2.extras import Json

class PostgresTestDB:
    def __init__(self, host='localhost', database='test_automation'):
        self.conn = psycopg2.connect(
            host=host,
            database=database,
            user='test_user',
            password='password'
        )
    
    def save_test_run(self, test_name, status, duration, metadata=None):
        cursor = self.conn.cursor()
        cursor.execute("""
            INSERT INTO test_runs (test_name, status, duration, metadata)
            VALUES (%s, %s, %s, %s)
            RETURNING id
        """, (test_name, status, duration, Json(metadata or {})))
        self.conn.commit()
        return cursor.fetchone()[0]
    
    def search_tests(self, keyword):
        cursor = self.conn.cursor()
        cursor.execute("""
            SELECT test_name, status, timestamp
            FROM test_runs
            WHERE test_name ILIKE %s OR error_message ILIKE %s
            ORDER BY timestamp DESC
            LIMIT 50
        """, (f'%{keyword}%', f'%{keyword}%'))
        return cursor.fetchall()
```

#### Cost
- **Self-hosted:** FREE (server costs only)
- **Managed (AWS RDS):** $15-200+/month
- **Managed (DigitalOcean):** $15-50/month

---

### 3. **MongoDB** üçÉ

#### Overview
NoSQL document database, stores JSON-like documents.

#### Pros ‚úÖ
- **Schema-less** - Flexible structure
- **JSON native** - Perfect for test data
- **Easy to scale** - Horizontal sharding
- **Fast development** - No migrations
- **Rich queries** - Complex document queries
- **Built-in aggregation** - Analytics pipeline
- **Good for logs** - Time-series data
- **Python-friendly** - PyMongo library

#### Cons ‚ùå
- **No ACID (by default)** - Eventual consistency
- **Memory intensive** - Keeps working set in RAM
- **No joins** - Must denormalize
- **Disk space** - Uses more than SQL
- **Learning curve** - Different query language

#### Best For
‚úÖ Rapidly changing schema  
‚úÖ Large volumes of logs  
‚úÖ Document-centric data  
‚úÖ Microservices  
‚úÖ Real-time analytics

#### Schema Example (Collections)
```javascript
// test_runs collection
{
  "_id": ObjectId("..."),
  "test_name": "Search Functionality",
  "timestamp": ISODate("2025-10-17T10:30:00Z"),
  "status": "pass",
  "duration": 45.2,
  "steps": [
    {
      "step_number": 1,
      "action": "navigation",
      "success": true,
      "screenshot_id": ObjectId("...")
    },
    {
      "step_number": 2,
      "action": "click",
      "success": true,
      "element": {
        "tagName": "BUTTON",
        "id": "search-btn"
      }
    }
  ],
  "metadata": {
    "browser": "chrome",
    "viewport": "1920x1080",
    "user": "shreyash"
  }
}

// screenshots collection (GridFS for large files)
{
  "_id": ObjectId("..."),
  "test_run_id": ObjectId("..."),
  "filename": "step_1_before.png",
  "upload_date": ISODate("..."),
  "content_type": "image/png",
  "metadata": {
    "step": 1,
    "type": "before"
  }
}
```

#### Code Example
```python
from pymongo import MongoClient
from datetime import datetime
import gridfs

class MongoTestDB:
    def __init__(self, connection_string='mongodb://localhost:27017/'):
        self.client = MongoClient(connection_string)
        self.db = self.client.test_automation
        self.fs = gridfs.GridFS(self.db)
    
    def save_test_run(self, test_data):
        test_data['timestamp'] = datetime.now()
        result = self.db.test_runs.insert_one(test_data)
        return result.inserted_id
    
    def save_screenshot(self, image_bytes, test_run_id, metadata):
        file_id = self.fs.put(
            image_bytes,
            content_type='image/png',
            test_run_id=test_run_id,
            metadata=metadata
        )
        return file_id
    
    def get_flaky_tests(self, days=7):
        pipeline = [
            {
                "$match": {
                    "timestamp": {
                        "$gte": datetime.now() - timedelta(days=days)
                    }
                }
            },
            {
                "$group": {
                    "_id": "$test_name",
                    "total": {"$sum": 1},
                    "passes": {
                        "$sum": {"$cond": [{"$eq": ["$status", "pass"]}, 1, 0]}
                    },
                    "fails": {
                        "$sum": {"$cond": [{"$eq": ["$status", "fail"]}, 1, 0]}
                    }
                }
            },
            {
                "$match": {
                    "passes": {"$gt": 0},
                    "fails": {"$gt": 0}
                }
            },
            {
                "$sort": {"fails": -1}
            }
        ]
        return list(self.db.test_runs.aggregate(pipeline))
```

#### Cost
- **Self-hosted:** FREE (server costs only)
- **MongoDB Atlas (Managed):** FREE tier ‚Üí $57+/month

---

### 4. **MySQL** üê¨

#### Overview
Popular open-source relational database.

#### Pros ‚úÖ
- **Very popular** - Huge community
- **Mature** - 25+ years development
- **Fast reads** - Excellent read performance
- **Easy replication** - Master-slave setup
- **Good tools** - MySQL Workbench, phpMyAdmin
- **Widespread hosting** - Available everywhere
- **JSON support** - Native JSON columns

#### Cons ‚ùå
- **Less features than Postgres** - Fewer data types
- **Licensing concerns** - Oracle ownership
- **Weaker concurrency** - Table-level locking (InnoDB better)
- **Less standards compliant** - Some SQL quirks

#### Best For
‚úÖ Web applications  
‚úÖ Read-heavy workloads  
‚úÖ Traditional hosting  
‚úÖ Budget constraints

#### Cost
Similar to PostgreSQL

---

### 5. **Supabase** ‚ö° (PostgreSQL + Storage + API)

#### Overview
Open-source Firebase alternative, PostgreSQL + real-time + storage + auth.

#### Pros ‚úÖ
- **All-in-one** - DB + Storage + Auth + API
- **Auto-generated API** - REST + GraphQL instantly
- **Real-time** - Live updates
- **Easy setup** - No backend code needed
- **Great free tier** - 500MB database, 1GB storage
- **Row-level security** - Built-in access control
- **Dashboard** - Beautiful web UI
- **PostgreSQL** - Full SQL power

#### Cons ‚ùå
- **Vendor lock-in** - Proprietary platform
- **Less control** - Managed service limitations
- **Costs scale** - Can get expensive
- **Still maturing** - Newer platform

#### Best For
‚úÖ Rapid development  
‚úÖ Need API + DB + Storage  
‚úÖ Team collaboration  
‚úÖ Remote access  
‚úÖ Modern web apps

#### Code Example
```python
from supabase import create_client, Client

class SupabaseTestDB:
    def __init__(self, url: str, key: str):
        self.supabase: Client = create_client(url, key)
    
    def save_test_run(self, test_data):
        result = self.supabase.table('test_runs').insert(test_data).execute()
        return result.data[0]['id']
    
    def upload_screenshot(self, file_path, bucket='screenshots'):
        with open(file_path, 'rb') as f:
            result = self.supabase.storage.from_(bucket).upload(
                file_path,
                f
            )
        return result
    
    def get_test_history(self, test_name, limit=10):
        result = self.supabase.table('test_runs')\
            .select('*')\
            .eq('test_name', test_name)\
            .order('timestamp', desc=True)\
            .limit(limit)\
            .execute()
        return result.data
```

#### Cost
- **Free tier:** 500MB DB, 1GB storage, 2GB bandwidth
- **Pro:** $25/month - 8GB DB, 100GB storage
- **Team:** $599/month - dedicated instance

---

### 6. **TinyDB** üìù (Document-based, Python)

#### Overview
Lightweight document-oriented database in pure Python.

#### Pros ‚úÖ
- **Pure Python** - No external dependencies
- **Zero config** - Single JSON file
- **Simple API** - Very easy to use
- **Perfect for prototypes** - Quick start
- **Readable storage** - JSON text file

#### Cons ‚ùå
- **Not production-ready** - Limited features
- **No concurrency** - Single file locking
- **Slow for large data** - Loads entire DB in memory
- **No transactions** - Risk of data loss

#### Best For
‚úÖ Quick prototypes  
‚úÖ Very small datasets  
‚úÖ Learning/experimentation  
‚ùå NOT for production

#### Code Example
```python
from tinydb import TinyDB, Query

db = TinyDB('test_database.json')
test_runs = db.table('test_runs')

# Insert test run
test_runs.insert({
    'test_name': 'Search Test',
    'status': 'pass',
    'duration': 45.2
})

# Query
Test = Query()
results = test_runs.search(Test.status == 'fail')
```

#### Cost
**FREE** ‚úÖ

---

## Photo/Screenshot Storage Comparison

### Current Issues
```
screenshots/
‚îú‚îÄ‚îÄ step_1_before_20251017_145955.png (2.3 MB)
‚îú‚îÄ‚îÄ step_1_after_20251017_150020.png (2.1 MB)
‚îú‚îÄ‚îÄ step_2_before_20251017_150021.png (2.4 MB)
...

Problems:
‚ùå Fills up disk quickly (2-3 MB per screenshot)
‚ùå Hard to manage thousands of files
‚ùå No easy sharing with team
‚ùå No automatic cleanup
‚ùå Expensive to backup
‚ùå Slow to load in reports
```

---

### 1. **Local File System** üìÅ

#### Pros ‚úÖ
- **Simple** - No setup
- **Fast access** - No network latency
- **Free** - Use existing disk

#### Cons ‚ùå
- **Disk space** - 100 tests = ~500 MB
- **No sharing** - Can't access remotely
- **Manual cleanup** - No auto-deletion
- **Backup complexity** - Need separate solution
- **Scalability** - Limited by disk

#### Best For
‚úÖ Single developer  
‚úÖ Low volume  
‚úÖ Local development

#### Improvements
```python
# Organized structure
screenshots/
‚îú‚îÄ‚îÄ 2025/
‚îÇ   ‚îú‚îÄ‚îÄ 10/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ 17/
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ search_test_run_123/
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ step_1_before.png
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ step_1_after.png

# Auto-cleanup old screenshots
import os
from datetime import datetime, timedelta

def cleanup_old_screenshots(days=30):
    cutoff = datetime.now() - timedelta(days=days)
    for root, dirs, files in os.walk('screenshots'):
        for file in files:
            filepath = os.path.join(root, file)
            if os.path.getmtime(filepath) < cutoff.timestamp():
                os.remove(filepath)
```

#### Cost
**FREE** (disk space only)

---

### 2. **AWS S3** ‚òÅÔ∏è

#### Overview
Object storage service from Amazon Web Services.

#### Pros ‚úÖ
- **Unlimited storage** - Scales infinitely
- **Cheap** - $0.023/GB/month
- **Durable** - 99.999999999% durability
- **CDN integration** - CloudFront for fast delivery
- **Lifecycle policies** - Auto-delete old files
- **Versioning** - Keep file history
- **Security** - Fine-grained access control
- **Global** - Access from anywhere

#### Cons ‚ùå
- **AWS complexity** - Learning curve
- **Transfer costs** - $0.09/GB download
- **Requires account** - Setup overhead
- **Vendor lock-in** - AWS ecosystem

#### Best For
‚úÖ Production systems  
‚úÖ Large scale (>1000 screenshots/day)  
‚úÖ Team collaboration  
‚úÖ Long-term storage

#### Code Example
```python
import boto3
from datetime import datetime

class S3ScreenshotStorage:
    def __init__(self, bucket_name='test-screenshots'):
        self.s3 = boto3.client('s3')
        self.bucket = bucket_name
    
    def upload_screenshot(self, local_path, test_run_id, step):
        key = f"{datetime.now().strftime('%Y/%m/%d')}/{test_run_id}/step_{step}.png"
        self.s3.upload_file(
            local_path,
            self.bucket,
            key,
            ExtraArgs={
                'ContentType': 'image/png',
                'StorageClass': 'STANDARD_IA',  # Cheaper for infrequent access
                'Metadata': {
                    'test_run_id': str(test_run_id),
                    'step': str(step)
                }
            }
        )
        # Generate pre-signed URL (expires in 7 days)
        url = self.s3.generate_presigned_url(
            'get_object',
            Params={'Bucket': self.bucket, 'Key': key},
            ExpiresIn=604800
        )
        return url
    
    def setup_lifecycle_policy(self):
        # Auto-delete files older than 90 days
        lifecycle_policy = {
            'Rules': [
                {
                    'Id': 'DeleteOldScreenshots',
                    'Status': 'Enabled',
                    'Expiration': {'Days': 90},
                    'Prefix': ''
                }
            ]
        }
        self.s3.put_bucket_lifecycle_configuration(
            Bucket=self.bucket,
            LifecycleConfiguration=lifecycle_policy
        )
```

#### Cost Example
```
Storage: 10,000 screenshots √ó 2 MB = 20 GB
Cost: 20 GB √ó $0.023/GB = $0.46/month

Requests: 10,000 PUT + 50,000 GET
Cost: (10,000 √ó $0.005/1000) + (50,000 √ó $0.0004/1000) = $0.07/month

Total: ~$0.53/month for 10k screenshots
```

---

### 3. **Cloudinary** üñºÔ∏è

#### Overview
Image/video management platform with transformations.

#### Pros ‚úÖ
- **Image optimization** - Auto-compress, resize
- **CDN included** - Fast global delivery
- **Transformations** - Resize on-the-fly
- **Easy API** - Simple integration
- **Face detection** - AI features
- **Video support** - If needed later
- **Backup** - Automatic backups

#### Cons ‚ùå
- **Expensive** - $0.05/GB (2x AWS)
- **Overkill** - Features we don't need
- **Bandwidth limits** - Extra costs

#### Best For
‚úÖ Need image transformations  
‚úÖ User-facing screenshots  
‚úÖ Marketing/demos

#### Code Example
```python
import cloudinary
import cloudinary.uploader

cloudinary.config(
    cloud_name="your_cloud_name",
    api_key="your_api_key",
    api_secret="your_api_secret"
)

def upload_screenshot(local_path, test_run_id):
    result = cloudinary.uploader.upload(
        local_path,
        folder=f"test_runs/{test_run_id}",
        tags=['screenshot', 'test'],
        resource_type='image'
    )
    return result['secure_url']

# Get optimized version
def get_thumbnail_url(public_id):
    return cloudinary.CloudinaryImage(public_id).build_url(
        width=400,
        height=300,
        crop='fill',
        quality='auto'
    )
```

#### Cost
- **Free tier:** 25 GB storage, 25 GB bandwidth
- **Plus:** $99/month - 100 GB storage, 100 GB bandwidth

---

### 4. **MinIO** üì¶ (Self-hosted S3)

#### Overview
Open-source S3-compatible object storage.

#### Pros ‚úÖ
- **S3 compatible** - Use AWS SDK
- **Self-hosted** - Full control
- **FREE** - No per-GB fees
- **High performance** - SSD optimized
- **Docker support** - Easy deployment

#### Cons ‚ùå
- **Self-managed** - Your responsibility
- **Server costs** - Need infrastructure
- **No built-in CDN** - Add yourself
- **Setup complexity** - More work

#### Best For
‚úÖ On-premise requirements  
‚úÖ Cost-sensitive (high volume)  
‚úÖ Full control needed  
‚úÖ Kubernetes deployment

#### Docker Setup
```bash
docker run -p 9000:9000 -p 9001:9001 \
  -e MINIO_ROOT_USER=admin \
  -e MINIO_ROOT_PASSWORD=password123 \
  minio/minio server /data --console-address ":9001"
```

#### Code Example
```python
from minio import Minio

class MinIOStorage:
    def __init__(self):
        self.client = Minio(
            "localhost:9000",
            access_key="admin",
            secret_key="password123",
            secure=False
        )
        self.bucket = "screenshots"
        
        # Create bucket if not exists
        if not self.client.bucket_exists(self.bucket):
            self.client.make_bucket(self.bucket)
    
    def upload_screenshot(self, local_path, object_name):
        self.client.fput_object(
            self.bucket,
            object_name,
            local_path,
            content_type='image/png'
        )
        return f"http://localhost:9000/{self.bucket}/{object_name}"
```

#### Cost
- **Software:** FREE
- **Server:** $5-50/month (DigitalOcean, Hetzner)

---

### 5. **Supabase Storage** üì§

#### Overview
S3-compatible storage integrated with Supabase.

#### Pros ‚úÖ
- **Integrated** - Same as DB solution
- **Simple API** - Easy to use
- **CDN included** - Fast delivery
- **Access control** - Row-level security
- **Transformations** - Image resize

#### Cons ‚ùå
- **Vendor lock-in** - Supabase only
- **Limited free tier** - 1GB storage

#### Code Example
```python
# Upload to Supabase storage
def upload_screenshot(self, file_path, test_run_id):
    with open(file_path, 'rb') as f:
        result = self.supabase.storage.from_('screenshots').upload(
            f'{test_run_id}/{os.path.basename(file_path)}',
            f,
            file_options={'content-type': 'image/png'}
        )
    
    # Get public URL
    url = self.supabase.storage.from_('screenshots').get_public_url(
        f'{test_run_id}/{os.path.basename(file_path)}'
    )
    return url
```

#### Cost
Included in Supabase pricing (see Database section)

---

## Recommended Architectures

### Option 1: **Simple & Free** (Best for Solo Developer)

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ         SQLite Database             ‚îÇ
‚îÇ  test_automation.db (local file)    ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ - Test runs                         ‚îÇ
‚îÇ - Test steps                        ‚îÇ
‚îÇ - Screenshot metadata               ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
              ‚îÇ
              ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ    Local File System Storage        ‚îÇ
‚îÇ  screenshots/ (organized folders)   ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ - 2025/10/17/test_123/step_1.png   ‚îÇ
‚îÇ - Auto-cleanup after 30 days        ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

Benefits:
‚úÖ Zero cost
‚úÖ Easy setup (5 minutes)
‚úÖ Fast performance
‚úÖ No external dependencies
‚úÖ Perfect for local dev

Limitations:
‚ùå Single user
‚ùå No remote access
‚ùå Manual backups needed
```

---

### Option 2: **Professional** (Best for Small-Medium Teams)

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ      PostgreSQL Database            ‚îÇ
‚îÇ  (DigitalOcean Managed DB)          ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ - Test runs & results               ‚îÇ
‚îÇ - User management                   ‚îÇ
‚îÇ - Test analytics                    ‚îÇ
‚îÇ - API access                        ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
              ‚îÇ
              ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ         AWS S3 Storage              ‚îÇ
‚îÇ  (with lifecycle policies)          ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ - Screenshots (2MB each)            ‚îÇ
‚îÇ - Auto-delete after 90 days         ‚îÇ
‚îÇ - CDN delivery                      ‚îÇ
‚îÇ - Presigned URLs                    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

Benefits:
‚úÖ Multi-user support
‚úÖ Remote access
‚úÖ Automatic backups
‚úÖ Scalable
‚úÖ Professional

Cost: ~$30/month
- PostgreSQL: $15/month (DigitalOcean)
- S3: ~$2-10/month (depending on volume)
- Total: $17-25/month
```

---

### Option 3: **All-in-One Modern** (Best for Rapid Development)

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ          Supabase                   ‚îÇ
‚îÇ  (All-in-one platform)              ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ ‚úì PostgreSQL database               ‚îÇ
‚îÇ ‚úì Object storage                    ‚îÇ
‚îÇ ‚úì Auto-generated API                ‚îÇ
‚îÇ ‚úì Real-time updates                 ‚îÇ
‚îÇ ‚úì Authentication (if needed)        ‚îÇ
‚îÇ ‚úì Dashboard UI                      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

Benefits:
‚úÖ Everything integrated
‚úÖ Auto-generated REST/GraphQL API
‚úÖ Beautiful dashboard
‚úÖ Real-time capabilities
‚úÖ Quick setup (15 minutes)
‚úÖ Team collaboration built-in

Cost:
- Free tier: 500MB DB, 1GB storage (good for starting)
- Pro: $25/month (8GB DB, 100GB storage)
```

---

### Option 4: **Enterprise Self-Hosted** (Best for Large Teams/On-Premise)

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ      PostgreSQL Cluster             ‚îÇ
‚îÇ  (Self-hosted, replicated)          ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ - Primary + 2 replicas              ‚îÇ
‚îÇ - Connection pooling                ‚îÇ
‚îÇ - Automated backups                 ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
              ‚îÇ
              ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ         MinIO Cluster               ‚îÇ
‚îÇ  (S3-compatible, self-hosted)       ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ - Distributed storage               ‚îÇ
‚îÇ - Erasure coding                    ‚îÇ
‚îÇ - Encryption                        ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

Benefits:
‚úÖ Full control
‚úÖ Data sovereignty
‚úÖ No per-GB costs
‚úÖ Unlimited scale
‚úÖ Compliance ready

Cost: $100-500+/month (server costs)
```

---

## Cost Analysis (12-month projection)

### Scenario: 50 tests/day, 2 screenshots/test, 2MB/screenshot

**Data Volume:**
- 50 tests √ó 365 days = 18,250 tests/year
- 18,250 √ó 2 screenshots = 36,500 screenshots
- 36,500 √ó 2 MB = 73 GB/year
- Database: ~5 GB (metadata)

### Option 1: SQLite + Local Storage
```
Cost: $0/month
Total year 1: $0

Pros: Free, fast, simple
Cons: Single user, no backup, disk space
```

### Option 2: PostgreSQL (DO) + AWS S3
```
PostgreSQL: $15/month
S3 Storage: 73 GB √ó $0.023 = $1.68/month
S3 Requests: ~$2/month
Total: $18.68/month √ó 12 = $224/year

Pros: Professional, scalable, team-ready
Cons: Monthly cost
```

### Option 3: Supabase (Pro)
```
Cost: $25/month √ó 12 = $300/year
Includes: 8GB DB + 100GB storage + API + Auth

Pros: All-in-one, easy, modern
Cons: Higher cost, vendor lock-in
```

### Option 4: Self-Hosted (MinIO + PostgreSQL)
```
Server (DigitalOcean 4GB): $24/month
Total: $24/month √ó 12 = $288/year

Pros: Full control, unlimited scale
Cons: Setup/maintenance time
```

---

## Implementation Examples

### Example 1: SQLite + Local Storage (Simplest)

```python
# database.py
import sqlite3
import os
from datetime import datetime
from pathlib import Path

class TestDatabase:
    def __init__(self, db_path='test_automation.db', screenshot_dir='screenshots'):
        self.db_path = db_path
        self.screenshot_dir = Path(screenshot_dir)
        self.conn = sqlite3.connect(db_path)
        self.create_tables()
        self.screenshot_dir.mkdir(exist_ok=True)
    
    def create_tables(self):
        cursor = self.conn.cursor()
        
        # Test runs table
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS test_runs (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                test_name TEXT NOT NULL,
                timestamp DATETIME DEFAULT CURRENT_TIMESTAMP,
                status TEXT NOT NULL,
                duration REAL,
                error_message TEXT,
                browser TEXT,
                viewport TEXT
            )
        ''')
        
        # Test steps table
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS test_steps (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                test_run_id INTEGER NOT NULL,
                step_number INTEGER NOT NULL,
                action TEXT NOT NULL,
                success BOOLEAN NOT NULL,
                error_message TEXT,
                method TEXT,
                FOREIGN KEY (test_run_id) REFERENCES test_runs(id)
            )
        ''')
        
        # Screenshots table
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS screenshots (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                test_run_id INTEGER NOT NULL,
                step_number INTEGER NOT NULL,
                type TEXT NOT NULL,
                file_path TEXT NOT NULL,
                file_size INTEGER,
                captured_at DATETIME DEFAULT CURRENT_TIMESTAMP,
                FOREIGN KEY (test_run_id) REFERENCES test_runs(id)
            )
        ''')
        
        # Indexes for performance
        cursor.execute('CREATE INDEX IF NOT EXISTS idx_test_runs_timestamp ON test_runs(timestamp)')
        cursor.execute('CREATE INDEX IF NOT EXISTS idx_test_runs_status ON test_runs(status)')
        cursor.execute('CREATE INDEX IF NOT EXISTS idx_test_steps_run ON test_steps(test_run_id)')
        
        self.conn.commit()
    
    def save_test_run(self, test_name, status, duration, error=None, browser='chrome', viewport='1920x1080'):
        """Save a test run and return its ID"""
        cursor = self.conn.cursor()
        cursor.execute('''
            INSERT INTO test_runs (test_name, status, duration, error_message, browser, viewport)
            VALUES (?, ?, ?, ?, ?, ?)
        ''', (test_name, status, duration, error, browser, viewport))
        self.conn.commit()
        return cursor.lastrowid
    
    def save_test_step(self, test_run_id, step_number, action, success, error=None, method=None):
        """Save a test step"""
        cursor = self.conn.cursor()
        cursor.execute('''
            INSERT INTO test_steps (test_run_id, step_number, action, success, error_message, method)
            VALUES (?, ?, ?, ?, ?, ?)
        ''', (test_run_id, step_number, action, success, error, method))
        self.conn.commit()
        return cursor.lastrowid
    
    def save_screenshot(self, test_run_id, step_number, screenshot_type, source_path):
        """Save screenshot metadata and organize file"""
        # Create organized directory structure
        date_path = datetime.now().strftime('%Y/%m/%d')
        dest_dir = self.screenshot_dir / date_path / f'test_{test_run_id}'
        dest_dir.mkdir(parents=True, exist_ok=True)
        
        # Move screenshot to organized location
        filename = f'step_{step_number}_{screenshot_type}.png'
        dest_path = dest_dir / filename
        
        if os.path.exists(source_path):
            os.rename(source_path, dest_path)
            file_size = dest_path.stat().st_size
        else:
            return None
        
        # Save metadata to database
        cursor = self.conn.cursor()
        relative_path = str(dest_path.relative_to(self.screenshot_dir))
        cursor.execute('''
            INSERT INTO screenshots (test_run_id, step_number, type, file_path, file_size)
            VALUES (?, ?, ?, ?, ?)
        ''', (test_run_id, step_number, screenshot_type, relative_path, file_size))
        self.conn.commit()
        return cursor.lastrowid
    
    def get_test_history(self, test_name, limit=10):
        """Get recent runs of a specific test"""
        cursor = self.conn.cursor()
        cursor.execute('''
            SELECT id, timestamp, status, duration, error_message
            FROM test_runs
            WHERE test_name = ?
            ORDER BY timestamp DESC
            LIMIT ?
        ''', (test_name, limit))
        return cursor.fetchall()
    
    def get_flaky_tests(self, days=7, min_runs=5):
        """Find tests with inconsistent results"""
        cursor = self.conn.cursor()
        cursor.execute('''
            SELECT 
                test_name,
                COUNT(*) as total_runs,
                SUM(CASE WHEN status='pass' THEN 1 ELSE 0 END) as passes,
                SUM(CASE WHEN status='fail' THEN 1 ELSE 0 END) as fails,
                ROUND(SUM(CASE WHEN status='fail' THEN 1 ELSE 0 END) * 100.0 / COUNT(*), 2) as failure_rate
            FROM test_runs
            WHERE timestamp > datetime('now', '-' || ? || ' days')
            GROUP BY test_name
            HAVING COUNT(*) >= ?
                AND passes > 0 
                AND fails > 0
            ORDER BY failure_rate DESC
        ''', (days, min_runs))
        return cursor.fetchall()
    
    def get_test_statistics(self):
        """Get overall test statistics"""
        cursor = self.conn.cursor()
        cursor.execute('''
            SELECT 
                COUNT(*) as total_runs,
                SUM(CASE WHEN status='pass' THEN 1 ELSE 0 END) as passes,
                SUM(CASE WHEN status='fail' THEN 1 ELSE 0 END) as fails,
                AVG(duration) as avg_duration,
                COUNT(DISTINCT test_name) as unique_tests
            FROM test_runs
        ''')
        return cursor.fetchone()
    
    def cleanup_old_data(self, days=90):
        """Remove old test runs and screenshots"""
        cursor = self.conn.cursor()
        
        # Get old test runs
        cursor.execute('''
            SELECT id FROM test_runs
            WHERE timestamp < datetime('now', '-' || ? || ' days')
        ''', (days,))
        old_runs = cursor.fetchall()
        
        # Delete associated screenshots from disk
        for (run_id,) in old_runs:
            cursor.execute('SELECT file_path FROM screenshots WHERE test_run_id = ?', (run_id,))
            for (file_path,) in cursor.fetchall():
                full_path = self.screenshot_dir / file_path
                if full_path.exists():
                    full_path.unlink()
        
        # Delete from database
        cursor.execute('DELETE FROM screenshots WHERE test_run_id IN (SELECT id FROM test_runs WHERE timestamp < datetime("now", "-" || ? || " days"))', (days,))
        cursor.execute('DELETE FROM test_steps WHERE test_run_id IN (SELECT id FROM test_runs WHERE timestamp < datetime("now", "-" || ? || " days"))', (days,))
        cursor.execute('DELETE FROM test_runs WHERE timestamp < datetime("now", "-" || ? || " days")', (days,))
        
        self.conn.commit()
        return len(old_runs)

# Usage in replay_browser_activities.py
class BrowserReplayer:
    def __init__(self, activity_log_path: str = "activity_log.json"):
        self.db = TestDatabase()  # Add this
        # ... existing code ...
    
    def replay_activities(self, activities: List[Dict[str, Any]]):
        # Start test run
        test_run_id = self.db.save_test_run(
            test_name="Browser Activity Replay",
            status="running",
            duration=0
        )
        
        # ... existing replay code ...
        
        for i, activity in enumerate(activities, 1):
            result = self.executor.execute_activity(activity)
            
            # Save step to database
            self.db.save_test_step(
                test_run_id=test_run_id,
                step_number=i,
                action=result['action'],
                success=result['success'],
                error=result.get('error'),
                method=result.get('method')
            )
            
            # Save screenshots
            if 'screenshot_before' in result:
                self.db.save_screenshot(
                    test_run_id,
                    i,
                    'before',
                    result['screenshot_before']
                )
            if 'screenshot_after' in result:
                self.db.save_screenshot(
                    test_run_id,
                    i,
                    'after',
                    result['screenshot_after']
                )
        
        # Update test run with final status
        duration = (self.end_time - self.start_time).total_seconds()
        status = "pass" if all(r['success'] for r in self.results) else "fail"
        
        cursor = self.db.conn.cursor()
        cursor.execute('''
            UPDATE test_runs 
            SET status = ?, duration = ?
            WHERE id = ?
        ''', (status, duration, test_run_id))
        self.db.conn.commit()
```

### Example 2: PostgreSQL + AWS S3 (Production)

```python
# database.py
import psycopg2
from psycopg2.extras import RealDictCursor
import boto3
from datetime import datetime
import os

class ProductionTestDatabase:
    def __init__(self, db_config, s3_bucket):
        self.conn = psycopg2.connect(**db_config)
        self.s3 = boto3.client('s3')
        self.bucket = s3_bucket
        self.create_tables()
    
    def create_tables(self):
        with self.conn.cursor() as cursor:
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS test_runs (
                    id SERIAL PRIMARY KEY,
                    test_name VARCHAR(255) NOT NULL,
                    timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    status VARCHAR(50) NOT NULL,
                    duration NUMERIC(10, 2),
                    error_message TEXT,
                    metadata JSONB,
                    created_by VARCHAR(100),
                    environment VARCHAR(50)
                );
                
                CREATE INDEX IF NOT EXISTS idx_test_runs_timestamp ON test_runs(timestamp);
                CREATE INDEX IF NOT EXISTS idx_test_runs_status ON test_runs(status);
                CREATE INDEX IF NOT EXISTS idx_test_runs_metadata ON test_runs USING GIN(metadata);
                
                CREATE TABLE IF NOT EXISTS test_steps (
                    id SERIAL PRIMARY KEY,
                    test_run_id INTEGER REFERENCES test_runs(id) ON DELETE CASCADE,
                    step_number INTEGER NOT NULL,
                    action VARCHAR(100) NOT NULL,
                    success BOOLEAN NOT NULL,
                    error_message TEXT,
                    method VARCHAR(50),
                    duration NUMERIC(10, 2)
                );
                
                CREATE TABLE IF NOT EXISTS screenshots (
                    id SERIAL PRIMARY KEY,
                    test_run_id INTEGER REFERENCES test_runs(id) ON DELETE CASCADE,
                    step_number INTEGER NOT NULL,
                    type VARCHAR(50) NOT NULL,
                    s3_key TEXT NOT NULL,
                    s3_url TEXT NOT NULL,
                    file_size INTEGER,
                    captured_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                );
            ''')
            self.conn.commit()
    
    def save_test_run(self, test_name, metadata=None, created_by='system', environment='production'):
        with self.conn.cursor(cursor_factory=RealDictCursor) as cursor:
            cursor.execute('''
                INSERT INTO test_runs (test_name, status, metadata, created_by, environment)
                VALUES (%s, %s, %s, %s, %s)
                RETURNING id
            ''', (test_name, 'running', psycopg2.extras.Json(metadata or {}), created_by, environment))
            self.conn.commit()
            return cursor.fetchone()['id']
    
    def upload_screenshot_to_s3(self, local_path, test_run_id, step_number, screenshot_type):
        """Upload screenshot to S3 and return URL"""
        date_path = datetime.now().strftime('%Y/%m/%d')
        s3_key = f"{date_path}/test_{test_run_id}/step_{step_number}_{screenshot_type}.png"
        
        # Upload to S3
        self.s3.upload_file(
            local_path,
            self.bucket,
            s3_key,
            ExtraArgs={
                'ContentType': 'image/png',
                'StorageClass': 'STANDARD_IA',  # Cheaper for infrequent access
                'Metadata': {
                    'test_run_id': str(test_run_id),
                    'step': str(step_number),
                    'type': screenshot_type
                }
            }
        )
        
        # Generate presigned URL (valid for 7 days)
        url = self.s3.generate_presigned_url(
            'get_object',
            Params={'Bucket': self.bucket, 'Key': s3_key},
            ExpiresIn=604800  # 7 days
        )
        
        # Save metadata to database
        file_size = os.path.getsize(local_path)
        with self.conn.cursor(cursor_factory=RealDictCursor) as cursor:
            cursor.execute('''
                INSERT INTO screenshots (test_run_id, step_number, type, s3_key, s3_url, file_size)
                VALUES (%s, %s, %s, %s, %s, %s)
                RETURNING id
            ''', (test_run_id, step_number, screenshot_type, s3_key, url, file_size))
            self.conn.commit()
            return cursor.fetchone()['id']
    
    def setup_s3_lifecycle(self):
        """Configure S3 to auto-delete old screenshots"""
        lifecycle_config = {
            'Rules': [
                {
                    'Id': 'DeleteOldScreenshots',
                    'Status': 'Enabled',
                    'Expiration': {'Days': 90},
                    'Prefix': ''
                }
            ]
        }
        self.s3.put_bucket_lifecycle_configuration(
            Bucket=self.bucket,
            LifecycleConfiguration=lifecycle_config
        )

# config.py
DB_CONFIG = {
    'host': 'your-db.digitalocean.com',
    'database': 'test_automation',
    'user': 'test_user',
    'password': 'your_password',
    'port': 25060
}

S3_BUCKET = 'your-test-screenshots-bucket'

# Usage
db = ProductionTestDatabase(DB_CONFIG, S3_BUCKET)
test_run_id = db.save_test_run("Search Test", metadata={'browser': 'chrome'})
db.upload_screenshot_to_s3('screenshot.png', test_run_id, 1, 'before')
```

---

## Migration Strategy

### Phase 1: Add Database Layer (Week 1)
1. Choose database solution (recommend SQLite to start)
2. Create database schema
3. Add database class to codebase
4. Test with new recordings

### Phase 2: Migrate Screenshot Storage (Week 2)
1. Choose storage solution
2. Update screenshot handling code
3. Implement organized folder structure or cloud upload
4. Test uploads and retrievals

### Phase 3: Update Reporting (Week 3)
1. Query database for test history
2. Add trend graphs
3. Show flaky test detection
4. Generate better reports

### Phase 4: Add Analytics (Week 4)
1. Test success rate over time
2. Average duration tracking
3. Most failing tests
4. Performance trends

---

## My Recommendation

### For Your Current Stage: **Option 1 (SQLite + Local Storage)**

**Why:**
1. ‚úÖ **Zero cost** - Perfect for starting
2. ‚úÖ **5-minute setup** - Get going immediately
3. ‚úÖ **Easy to upgrade later** - Can migrate to PostgreSQL
4. ‚úÖ **Fast development** - No cloud accounts needed
5. ‚úÖ **Fully functional** - All features work

**Upgrade Path:**
- **6 months:** If team grows ‚Üí PostgreSQL + S3
- **12 months:** If scaling ‚Üí Supabase or self-hosted

### Implementation Priority:
1. **This week:** Add SQLite database ‚úÖ
2. **Next week:** Organize screenshot storage ‚úÖ
3. **Month 2:** Add cleanup automation ‚úÖ
4. **Month 3:** Consider cloud migration if needed

---

## Quick Start Code

Save this as `test_database.py` and you're ready to go:

```python
# Copy the SQLite implementation from Example 1 above
# Then use it in your code:

from test_database import TestDatabase

# Initialize
db = TestDatabase()

# Save a test run
test_run_id = db.save_test_run(
    test_name="IBM Search Test",
    status="pass",
    duration=45.9
)

# Save steps
db.save_test_step(test_run_id, 1, "navigation", True)
db.save_test_step(test_run_id, 2, "click", True, method="xpath")
db.save_test_step(test_run_id, 3, "text_input", True, method="shadow_dom")

# Save screenshots
db.save_screenshot(test_run_id, 1, "before", "step_1_before.png")
db.save_screenshot(test_run_id, 1, "after", "step_1_after.png")

# Query test history
history = db.get_test_history("IBM Search Test", limit=10)

# Find flaky tests
flaky = db.get_flaky_tests(days=7)

# Get statistics
stats = db.get_test_statistics()

# Cleanup old data
deleted = db.cleanup_old_data(days=90)
```

---

**Ready to implement? Start with SQLite today, upgrade to cloud when needed!** üöÄ
